{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')   \n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def read_pos_and_neg_words(positive_filepath, negative_filepath):\n",
    "    with open(positive_filepath, 'r',) as positive_file:\n",
    "        positive_words = positive_file.read().splitlines()\n",
    "    with open(negative_filepath, 'r',encoding='latin-1') as negative_file:\n",
    "        negative_words = negative_file.read().splitlines()\n",
    "    return positive_words, negative_words\n",
    "\n",
    "\n",
    "positive_words, negative_words = read_pos_and_neg_words(\"../data/positive-words.txt\", \"../data/negative-words.txt\")\n",
    "\n",
    "\n",
    "# teachers\n",
    "def count_pos(review):\n",
    "    return sum(word in review for word in positive_words)\n",
    "\n",
    "def count_neg(review):\n",
    "    return sum(word in review for word in negative_words)\n",
    "\n",
    "def count_word_no(review):\n",
    "    return (1 if \"no\" in review.lower() else 0)\n",
    "\n",
    "def count_pronoun(review):\n",
    "    return sum(word in [\"i\", \"me\", \"my\", \"you\", \"your\"] for word in review.lower().split())\n",
    "\n",
    "def count_exclaimation(review):\n",
    "    return (1 if \"!\" in review else 0)\n",
    "\n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_capital_chars(text):\n",
    "    count=0\n",
    "    for i in text:\n",
    "        if i.isupper():\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))\n",
    "    \n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "def count_htags(text):\n",
    "    return len(re.findall(r'(#w[A-Za-z0-9]*)', text))\n",
    "\n",
    "def count_mentions(text):\n",
    "    x = re.findall(r'(@w[A-Za-z0-9]*)', text)\n",
    "    return len(x)\n",
    "\n",
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)\n",
    "\n",
    "\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "\n",
    "def filter_non_english(text):\n",
    "    from nltk.corpus import words\n",
    "    english_word_set = set(words.words())\n",
    "    words = text.split()\n",
    "    english_words = [word for word in words if word.lower() in english_word_set]\n",
    "    return ' '.join(english_words)\n",
    "\n",
    "def remove_stop_words_and_stem(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [stemmer.stem(word) for word in word_tokens]\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "                                \n",
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    sentence = replace_contractions(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    sentence = re.sub(r'\\w*\\d\\w*', '', sentence)\n",
    "    sentence = remove_stop_words_and_stem(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 14:43:12.229241: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-12 14:43:12.268509: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-12 14:43:12.268742: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-12 14:43:12.270445: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-12 14:43:12.270684: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-12 14:43:12.270900: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-12 14:43:12.337041: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-12 14:43:12.337252: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-12 14:43:12.337416: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-12 14:43:12.337537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4785 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " text (InputLayer)           [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " text_vectorization_1 (Text  (None, 500)                  0         ['text[0][0]']                \n",
      " Vectorization)                                                                                   \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 500, 256)             5120256   ['text_vectorization_1[0][0]']\n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 500, 256)             0         ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 12)]                 0         []                            \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 165, 128)             229504    ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 12)                   0         ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 53, 128)              114816    ['conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 64)                   832       ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Gl  (None, 128)                  0         ['conv1d_3[0][0]']            \n",
      " obalMaxPooling1D)                                                                                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 64)                   0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 192)                  0         ['global_max_pooling1d_1[0][0]\n",
      " )                                                                  ',                            \n",
      "                                                                     'dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 128)                  24704     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 128)                  0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " predictions (Dense)         (None, 1)                    129       ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5490241 (20.94 MB)\n",
      "Trainable params: 5490241 (20.94 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"../models/model\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raychannudam/miniconda3/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator OneHotEncoder from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"../models/encoder\", \"rb\") as f:\n",
    "    encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "[[0.731075  ]\n",
      " [0.26894116]\n",
      " [0.731075  ]\n",
      " [0.26894194]\n",
      " [0.73107356]\n",
      " [0.2689415 ]\n",
      " [0.26894137]\n",
      " [0.7310042 ]\n",
      " [0.26894116]\n",
      " [0.7310653 ]]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    \"The product exceeded my expectations! It's incredibly durable and efficient. I highly recommend it to anyone in need of such a tool.\",\n",
    "    \"I was disappointed with the quality of the product. It broke within the first week of use, and the customer service was unhelpful in resolving the issue.\",\n",
    "    \"This restaurant never fails to impress me. The food is always delicious, and the staff is friendly and attentive. Definitely worth a visit!\",\n",
    "    \"I regret purchasing this item. It doesn't work as advertised, and I feel like I wasted my money.\",\n",
    "    \"The movie was a masterpiece! The acting was superb, the storyline was captivating, and the cinematography was stunning. I can't wait to watch it again.\",\n",
    "    \"I had a terrible experience at this hotel. The room was dirty, the staff was rude, and there were constant disturbances throughout the night.\",\n",
    "    \"The book was a page-turner from start to finish. The characters were well-developed, and the plot kept me hooked until the very end.\",\n",
    "    \"I absolutely love this app! It's user-friendly, and it has all the features I need to stay organized and productive.\",\n",
    "    \"The concert was a disaster. The sound quality was terrible, and the performers seemed unenthusiastic. I left feeling disappointed and frustrated.\",\n",
    "    \"I'm impressed with the efficiency of this product. It's made my daily tasks so much easier to manage.\",\n",
    "]\n",
    "\n",
    "numerical_columns = [\"positive_word_count\", \n",
    "                     \"negative_word_count\", \n",
    "                     \"char_count\", \n",
    "                     \"word_count\", \n",
    "                     \"capital_char_count\", \n",
    "                     \"stopword_count\", \n",
    "                     \"unique_word_count\", \n",
    "                     \"pronounce_count\"]\n",
    "\n",
    "test_data = pd.DataFrame(data, columns=['reviews'])\n",
    "\n",
    "test_data['positive_word_count'] = test_data['reviews'].apply(count_pos)\n",
    "test_data['negative_word_count'] = test_data['reviews'].apply(count_neg)\n",
    "test_data['char_count'] = test_data[\"reviews\"].apply(count_chars)\n",
    "test_data['word_count'] = test_data[\"reviews\"].apply(count_words)\n",
    "test_data['capital_char_count'] = test_data[\"reviews\"].apply(count_capital_chars)\n",
    "test_data['stopword_count'] = test_data[\"reviews\"].apply(count_stopwords)\n",
    "test_data['unique_word_count'] = test_data[\"reviews\"].apply(count_unique_words)\n",
    "test_data['word_no'] = test_data['reviews'].apply(count_word_no)\n",
    "test_data['pronounce_count'] = test_data['reviews'].apply(count_pronoun)\n",
    "test_data['exclaimation'] = test_data['reviews'].apply(count_exclaimation)\n",
    "\n",
    "test_data['cleaned_reviews'] = test_data['reviews'].apply(preprocess_sentence)\n",
    "enc_data = pd.DataFrame(encoder.transform( \n",
    "    test_data[['word_no', 'exclaimation']]).toarray()) \n",
    "  \n",
    "# Merge with main \n",
    "test_data = test_data.join(enc_data)\n",
    "test_data.drop(columns=[\"word_no\", \"exclaimation\"], inplace=True)\n",
    "\n",
    "for header in numerical_columns:\n",
    "    test_data[header] = test_data[header].apply(lambda x: math.log(x+1))\n",
    "    \n",
    "y_predicted = model.predict([test_data.cleaned_reviews,test_data.drop(columns=['cleaned_reviews', 'reviews'])])\n",
    "y_predicted_norm = (y_predicted - np.mean(y_predicted, axis=0))/ np.std(y_predicted, axis=0)\n",
    "y_predicted_norm = 1 / (1 + np.exp(-y_predicted_norm))\n",
    "print(y_predicted_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
