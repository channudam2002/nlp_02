{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 13:11:55.580163: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-09 13:11:55.580287: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-09 13:11:55.642024: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-09 13:11:55.752667: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-09 13:11:56.903593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "translator = str.maketrans(\"\",\"\",string.punctuation)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "# create removeNewline function\n",
    "def removeNewline(content):\n",
    "    return re.sub(\"\\n\", \"\", content)\n",
    "\n",
    "# convert to lowercase\n",
    "def toLower(content):\n",
    "    return content.lower()\n",
    "\n",
    "# remove punctuation\n",
    "def removePunctuation(content):\n",
    "    return content.translate(translator)\n",
    "\n",
    "# word lemmatization\n",
    "def wordLemmatization(content):\n",
    "    words = nltk.word_tokenize(content)\n",
    "    tagged_words = pos_tag(words)\n",
    "    lemma = [lemmatizer.lemmatize(word, \"v\") if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] else word for word, pos in tagged_words]\n",
    "    return \" \".join(lemma)\n",
    "\n",
    "# remove single character\n",
    "def removeSingleChar(content):\n",
    "    return re.sub(r'\\b\\w{1}\\b', '', content)\n",
    "\n",
    "# remove pronounce\n",
    "def removePronouns(content):\n",
    "    words = nltk.word_tokenize(content)\n",
    "    tagged_words = pos_tag(words)\n",
    "    non_pronounces = [word for word, pos in tagged_words if pos not in ['PRP', 'PRP$', 'WP', 'WP$']]\n",
    "    return \" \".join(non_pronounces)\n",
    "\n",
    "# remove stopwords\n",
    "def removeStopwords(content):\n",
    "    words = []\n",
    "    words = content.split(\" \")\n",
    "    all_words = set(words)\n",
    "    common_words = set(words).intersection(set(STOP_WORDS))\n",
    "    uncommon_words = list(all_words - common_words)\n",
    "    return \" \".join(uncommon_words)\n",
    "\n",
    "# remove common\n",
    "def removeCommon(content):\n",
    "    with open(\"../data/common_word.txt\", \"r\") as f:\n",
    "        temp_common_words = f.readlines()\n",
    "        f.close()\n",
    "    common_words = [removeNewline(word) for word in temp_common_words]\n",
    "    words = set(content.split(\" \"))\n",
    "    common_words = set(common_words)\n",
    "    found_common_words = list(common_words.intersection(words))\n",
    "    uncommon_words = [word for word in content.split(\" \") if word not in found_common_words]\n",
    "    return \" \".join(uncommon_words)\n",
    "\n",
    "# count positive words\n",
    "def pos_count(content):\n",
    "    with open(\"../data/positive-reviews.txt\") as f:\n",
    "        temp_pos_words = f.readlines()\n",
    "        f.close()\n",
    "    postitive_words = [removeNewline(word) for word in temp_pos_words]\n",
    "    return len([word for word in content.split() if word in postitive_words])\n",
    "\n",
    "# count negative words\n",
    "def neg_count(content):\n",
    "    with open(\"../data/negative-reviews.txt\") as f:\n",
    "        temp_neg_words = f.readlines()\n",
    "        f.close()\n",
    "    negative_words = [removeNewline(word) for word in temp_neg_words]\n",
    "    return len([word for word in content.split() if word in negative_words])\n",
    "\n",
    "# check if contain 'no'\n",
    "def isContainNo(content):\n",
    "    return 1 if 'no' in content.split() else 0\n",
    "\n",
    "# check if contain '!'\n",
    "def isContainExclamation(content):\n",
    "    return 1 if '!' in content else 0\n",
    "\n",
    "# check if contain not \n",
    "def isContainNot(content):\n",
    "    return 1 if 'not' in content.split() else 0\n",
    "\n",
    "# check if contain but\n",
    "def isContainBut(content):\n",
    "    return 1 if 'but' in content.split() else 0\n",
    "\n",
    "# check if contain pronouns\n",
    "def pron_count(content):\n",
    "    return len([word for word in content.split() if word in ['i', 'me', 'my', 'you', 'your']])\n",
    "\n",
    "# get content length\n",
    "def getLength(content):\n",
    "    return np.log((len(content.split()))+1)\n",
    "\n",
    "# preprocessing\n",
    "def preprocessing_text(content):\n",
    "    content = toLower(content)\n",
    "    content = removePunctuation(content)\n",
    "    content = wordLemmatization(content)\n",
    "    content = removePronouns(content)\n",
    "    content = removeCommon(content)\n",
    "    return content\n",
    "\n",
    "# feature engineering\n",
    "def feature_engineering(data):\n",
    "    contents = []\n",
    "    for text in data:\n",
    "        content = toLower(text)\n",
    "        content = removeCommon(content)\n",
    "        contents.append(np.array([\n",
    "            pos_count(content),\n",
    "            neg_count(content),\n",
    "            isContainNo(content),\n",
    "            isContainNot(content),\n",
    "            isContainBut(content),\n",
    "            pron_count(content),\n",
    "            isContainExclamation(content),\n",
    "            getLength(content)\n",
    "        ]))\n",
    "    return contents\n",
    "\n",
    "def text_vectorize(data):\n",
    "    cleaned_text = []\n",
    "    for text in data:\n",
    "        cleaned_text.append(preprocessing_text(text))\n",
    "    from_file = pickle.load(open(\"../vectorizers/vectorizer.pkl\", \"rb\"))\n",
    "    vectorizer = TextVectorization.from_config(from_file['config'])\n",
    "    vectorizer.set_weights(from_file['weights'])\n",
    "    text = vectorizer(cleaned_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 13:12:03.510454: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-09 13:12:03.704913: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-09 13:12:03.705289: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-09 13:12:03.706362: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-09 13:12:03.706608: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-09 13:12:03.706842: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-09 13:12:03.785182: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-09 13:12:03.785465: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-09 13:12:03.785644: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-09 13:12:03.785810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4785 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-03-09 13:12:04.234503: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    \"The product exceeded my expectations! It's incredibly durable and efficient. I highly recommend it to anyone in need of such a tool.\",\n",
    "    \"I was disappointed with the quality of the product. It broke within the first week of use, and the customer service was unhelpful in resolving the issue.\",\n",
    "    \"This restaurant never fails to impress me. The food is always delicious, and the staff is friendly and attentive. Definitely worth a visit!\",\n",
    "    \"I regret purchasing this item. It doesn't work as advertised, and I feel like I wasted my money.\",\n",
    "    \"The movie was a masterpiece! The acting was superb, the storyline was captivating, and the cinematography was stunning. I can't wait to watch it again.\",\n",
    "    \"I had a terrible experience at this hotel. The room was dirty, the staff was rude, and there were constant disturbances throughout the night.\",\n",
    "    \"The book was a page-turner from start to finish. The characters were well-developed, and the plot kept me hooked until the very end.\",\n",
    "    \"I absolutely love this app! It's user-friendly, and it has all the features I need to stay organized and productive.\",\n",
    "    \"The concert was a disaster. The sound quality was terrible, and the performers seemed unenthusiastic. I left feeling disappointed and frustrated.\",\n",
    "    \"I'm impressed with the efficiency of this product. It's made my daily tasks so much easier to manage.\",\n",
    "]\n",
    "\n",
    "X_text = text_vectorize(data)\n",
    "X_num = np.array(feature_engineering(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 646ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-09 13:12:06.031106: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9999901 ],\n",
       "       [0.02369062],\n",
       "       [0.9999273 ],\n",
       "       [0.29117242],\n",
       "       [0.99189454],\n",
       "       [0.01319849],\n",
       "       [0.05512051],\n",
       "       [0.9999974 ],\n",
       "       [0.01917393],\n",
       "       [0.9999448 ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"../models/sentimental.h5\")\n",
    "model.predict([X_text, X_num])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
