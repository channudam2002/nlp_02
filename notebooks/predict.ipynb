{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/raychannudam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from keras.layers.experimental.preprocessing import TextVectorization\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "translator = str.maketrans(\"\",\"\",string.punctuation)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "# create removeNewline function\n",
    "def removeNewline(content):\n",
    "    return re.sub(\"\\n\", \"\", content)\n",
    "\n",
    "# convert to lowercase\n",
    "def toLower(content):\n",
    "    return content.lower()\n",
    "\n",
    "# remove punctuation\n",
    "def removePunctuation(content):\n",
    "    return content.translate(translator)\n",
    "\n",
    "# word lemmatization\n",
    "def wordLemmatization(content):\n",
    "    words = nltk.word_tokenize(content)\n",
    "    tagged_words = pos_tag(words)\n",
    "    lemma = [lemmatizer.lemmatize(word, \"v\") if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'] else word for word, pos in tagged_words]\n",
    "    return \" \".join(lemma)\n",
    "\n",
    "# remove single character\n",
    "def removeSingleChar(content):\n",
    "    return re.sub(r'\\b\\w{1}\\b', '', content)\n",
    "\n",
    "# remove pronounce\n",
    "def removePronouns(content):\n",
    "    words = nltk.word_tokenize(content)\n",
    "    tagged_words = pos_tag(words)\n",
    "    non_pronounces = [word for word, pos in tagged_words if pos not in ['PRP', 'PRP$', 'WP', 'WP$']]\n",
    "    return \" \".join(non_pronounces)\n",
    "\n",
    "# remove stopwords\n",
    "def removeStopwords(content):\n",
    "    words = []\n",
    "    words = content.split(\" \")\n",
    "    all_words = set(words)\n",
    "    common_words = set(words).intersection(set(STOP_WORDS))\n",
    "    uncommon_words = list(all_words - common_words)\n",
    "    return \" \".join(uncommon_words)\n",
    "\n",
    "# remove common\n",
    "def removeCommon(content):\n",
    "    with open(\"../data/common_word.txt\", \"r\") as f:\n",
    "        temp_common_words = f.readlines()\n",
    "        f.close()\n",
    "    common_words = [removeNewline(word) for word in temp_common_words]\n",
    "    words = set(content.split(\" \"))\n",
    "    common_words = set(common_words)\n",
    "    found_common_words = list(common_words.intersection(words))\n",
    "    uncommon_words = [word for word in content.split(\" \") if word not in found_common_words]\n",
    "    return \" \".join(uncommon_words)\n",
    "\n",
    "# count positive words\n",
    "def pos_count(content):\n",
    "    with open(\"../data/positive-reviews.txt\") as f:\n",
    "        temp_pos_words = f.readlines()\n",
    "        f.close()\n",
    "    postitive_words = [removeNewline(word) for word in temp_pos_words]\n",
    "    return len([word for word in content.split() if word in postitive_words])\n",
    "\n",
    "# count negative words\n",
    "def neg_count(content):\n",
    "    with open(\"../data/negative-reviews.txt\") as f:\n",
    "        temp_neg_words = f.readlines()\n",
    "        f.close()\n",
    "    negative_words = [removeNewline(word) for word in temp_neg_words]\n",
    "    return len([word for word in content.split() if word in negative_words])\n",
    "\n",
    "# check if contain 'no'\n",
    "def isContainNo(content):\n",
    "    return 1 if 'no' in content.split() else 0\n",
    "\n",
    "# check if contain '!'\n",
    "def isContainExclamation(content):\n",
    "    return 1 if '!' in content else 0\n",
    "\n",
    "# check if contain not \n",
    "def isContainNot(content):\n",
    "    return 1 if 'not' in content.split() else 0\n",
    "\n",
    "# check if contain but\n",
    "def isContainBut(content):\n",
    "    return 1 if 'but' in content.split() else 0\n",
    "\n",
    "# check if contain pronouns\n",
    "def pron_count(content):\n",
    "    return len([word for word in content.split() if word in ['i', 'me', 'my', 'you', 'your']])\n",
    "\n",
    "# get content length\n",
    "def getLength(content):\n",
    "    return np.log((len(content.split()))+1)\n",
    "\n",
    "# preprocessing\n",
    "def preprocessing_text(content):\n",
    "    content = toLower(content)\n",
    "    content = removePunctuation(content)\n",
    "    content = wordLemmatization(content)\n",
    "    content = removePronouns(content)\n",
    "    content = removeCommon(content)\n",
    "    return content\n",
    "\n",
    "# feature engineering\n",
    "def feature_engineering(data):\n",
    "    contents = []\n",
    "    for text in data:\n",
    "        content = toLower(text)\n",
    "        content = removeCommon(content)\n",
    "        contents.append(np.array([\n",
    "            pos_count(content),\n",
    "            neg_count(content),\n",
    "            isContainNo(content),\n",
    "            isContainNot(content),\n",
    "            isContainBut(content),\n",
    "            pron_count(content),\n",
    "            isContainExclamation(content),\n",
    "            getLength(content)\n",
    "        ]))\n",
    "    return contents\n",
    "\n",
    "def text_vectorize(data):\n",
    "    cleaned_text = []\n",
    "    for text in data:\n",
    "        cleaned_text.append(preprocessing_text(text))\n",
    "    from_file = pickle.load(open(\"../vectorizers/vectorizer.pkl\", \"rb\"))\n",
    "    vectorizer = TextVectorization.from_config(from_file['config'])\n",
    "    vectorizer.set_weights(from_file['weights'])\n",
    "    text = vectorizer(cleaned_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"The product exceeded my expectations! It's incredibly durable and efficient. I highly recommend it to anyone in need of such a tool.\",\n",
    "    \"I was disappointed with the quality of the product. It broke within the first week of use, and the customer service was unhelpful in resolving the issue.\",\n",
    "    \"This restaurant never fails to impress me. The food is always delicious, and the staff is friendly and attentive. Definitely worth a visit!\",\n",
    "    \"I regret purchasing this item. It doesn't work as advertised, and I feel like I wasted my money.\",\n",
    "    \"The movie was a masterpiece! The acting was superb, the storyline was captivating, and the cinematography was stunning. I can't wait to watch it again.\",\n",
    "    \"I had a terrible experience at this hotel. The room was dirty, the staff was rude, and there were constant disturbances throughout the night.\",\n",
    "    \"The book was a page-turner from start to finish. The characters were well-developed, and the plot kept me hooked until the very end.\",\n",
    "    \"I absolutely love this app! It's user-friendly, and it has all the features I need to stay organized and productive.\",\n",
    "    \"The concert was a disaster. The sound quality was terrible, and the performers seemed unenthusiastic. I left feeling disappointed and frustrated.\",\n",
    "    \"I'm impressed with the efficiency of this product. It's made my daily tasks so much easier to manage.\",\n",
    "]\n",
    "\n",
    "X_text = text_vectorize(data)\n",
    "X_num = np.array(feature_engineering(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f88dbfdf100> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 306ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9999901 ],\n",
       "       [0.02369062],\n",
       "       [0.9999273 ],\n",
       "       [0.29117242],\n",
       "       [0.99189454],\n",
       "       [0.01319849],\n",
       "       [0.05512051],\n",
       "       [0.9999974 ],\n",
       "       [0.01917393],\n",
       "       [0.9999448 ]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"../models/sentimental.h5\")\n",
    "model.predict([X_text, X_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
